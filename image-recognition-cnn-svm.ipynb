{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIE424 Trying to Run the Code",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import sys"
      ],
      "metadata": {
        "id": "3b6quUSDJ6zE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9743e41-47c0-47df-8440-1d34520437fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ],
      "metadata": {
        "id": "MzArDqbbKwSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model.cnn-svm\n",
        "\n",
        "#Class: CNNSVM"
      ],
      "metadata": {
        "id": "6G3mwfP4JwLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"2 Convolutional Layers with Max Pooling CNN\"\"\""
      ],
      "metadata": {
        "id": "NE0zVHzOKMhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNSVM:\n",
        "    def __init__(self, alpha, batch_size, num_classes, num_features, penalty_parameter):\n",
        "        \"\"\"Initializes the CNN-SVM model\n",
        "        :param alpha: The learning rate to be used by the model.\n",
        "        :param batch_size: The number of batches to use for training/validation/testing.\n",
        "        :param num_classes: The number of classes in the dataset.\n",
        "        :param num_features: The number of features in the dataset.\n",
        "        :param penalty_parameter: The SVM C penalty parameter.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.batch_size = batch_size\n",
        "        self.name = \"CNN-SVM\"\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = num_features\n",
        "        self.penalty_parameter = penalty_parameter\n",
        "\n",
        "        def __graph__():\n",
        "\n",
        "            with tf.name_scope(\"input\"):\n",
        "                # [BATCH_SIZE, NUM_FEATURES]\n",
        "                x_input = tf.placeholder(\n",
        "                    dtype=tf.float32, shape=[None, num_features], name=\"x_input\"\n",
        "                )\n",
        "\n",
        "                # [BATCH_SIZE, NUM_CLASSES]\n",
        "                y_input = tf.placeholder(\n",
        "                    dtype=tf.float32, shape=[None, num_classes], name=\"actual_label\"\n",
        "                )\n",
        "\n",
        "            # First convolutional layer\n",
        "            first_conv_weight = self.weight_variable([5, 5, 1, 32])\n",
        "            first_conv_bias = self.bias_variable([32])\n",
        "\n",
        "            input_image = tf.reshape(x_input, [-1, 28, 28, 1])\n",
        "\n",
        "            first_conv_activation = tf.nn.relu(\n",
        "                self.conv2d(input_image, first_conv_weight) + first_conv_bias\n",
        "            )\n",
        "            first_conv_pool = self.max_pool_2x2(first_conv_activation)\n",
        "\n",
        "            # Second convolutional layer\n",
        "            second_conv_weight = self.weight_variable([5, 5, 32, 64])\n",
        "            second_conv_bias = self.bias_variable([64])\n",
        "\n",
        "            second_conv_activation = tf.nn.relu(\n",
        "                self.conv2d(first_conv_pool, second_conv_weight) + second_conv_bias\n",
        "            )\n",
        "            second_conv_pool = self.max_pool_2x2(second_conv_activation)\n",
        "\n",
        "            # Fully-connected layer (Dense Layer)\n",
        "            dense_layer_weight = self.weight_variable([7 * 7 * 64, 1024])\n",
        "            dense_layer_bias = self.bias_variable([1024])\n",
        "\n",
        "            second_conv_pool_flatten = tf.reshape(second_conv_pool, [-1, 7 * 7 * 64])\n",
        "            dense_layer_activation = tf.nn.relu(\n",
        "                tf.matmul(second_conv_pool_flatten, dense_layer_weight)\n",
        "                + dense_layer_bias\n",
        "            )\n",
        "\n",
        "            # Dropout, to avoid over-fitting\n",
        "            keep_prob = tf.placeholder(tf.float32)\n",
        "            h_fc1_drop = tf.nn.dropout(dense_layer_activation, keep_prob)\n",
        "\n",
        "            # Readout layer\n",
        "            readout_weight = self.weight_variable([1024, num_classes])\n",
        "            readout_bias = self.bias_variable([num_classes])\n",
        "\n",
        "            output = tf.matmul(h_fc1_drop, readout_weight) + readout_bias\n",
        "\n",
        "            with tf.name_scope(\"svm\"):\n",
        "                regularization_loss = tf.reduce_mean(tf.square(readout_weight))\n",
        "                hinge_loss = tf.reduce_mean(\n",
        "                    tf.square(\n",
        "                        tf.maximum(\n",
        "                            tf.zeros([batch_size, num_classes]), 1 - y_input * output\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "                with tf.name_scope(\"loss\"):\n",
        "                    loss = regularization_loss + penalty_parameter * hinge_loss\n",
        "            tf.summary.scalar(\"loss\", loss)\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=alpha).minimize(loss)\n",
        "\n",
        "            with tf.name_scope(\"accuracy\"):\n",
        "                output = tf.identity(tf.sign(output), name=\"prediction\")\n",
        "                correct_prediction = tf.equal(\n",
        "                    tf.argmax(output, 1), tf.argmax(y_input, 1)\n",
        "                )\n",
        "                with tf.name_scope(\"accuracy\"):\n",
        "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "            tf.summary.scalar(\"accuracy\", accuracy)\n",
        "\n",
        "            merged = tf.summary.merge_all()\n",
        "\n",
        "            self.x_input = x_input\n",
        "            self.y_input = y_input\n",
        "            self.keep_prob = keep_prob\n",
        "            self.output = output\n",
        "            self.loss = loss\n",
        "            self.optimizer = optimizer\n",
        "            self.accuracy = accuracy\n",
        "            self.merged = merged\n",
        "\n",
        "        sys.stdout.write(\"\\n<log> Building graph...\")\n",
        "        __graph__()\n",
        "        sys.stdout.write(\"</log>\\n\")\n",
        "\n",
        "    def train(self, checkpoint_path, epochs, log_path, train_data, test_data):\n",
        "        \"\"\"Trains the initialized model.\n",
        "        :param checkpoint_path: The path where to save the trained model.\n",
        "        :param epochs: The number of passes through the entire dataset.\n",
        "        :param log_path: The path where to save the TensorBoard logs.\n",
        "        :param train_data: The training dataset.\n",
        "        :param test_data: The testing dataset.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        if not os.path.exists(path=log_path):\n",
        "            os.mkdir(log_path)\n",
        "\n",
        "        if not os.path.exists(path=checkpoint_path):\n",
        "            os.mkdir(checkpoint_path)\n",
        "\n",
        "        saver = tf.train.Saver(max_to_keep=4)\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        timestamp = str(time.asctime())\n",
        "\n",
        "        train_writer = tf.summary.FileWriter(\n",
        "            logdir=log_path + timestamp + \"-training\", graph=tf.get_default_graph()\n",
        "        )\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            checkpoint = tf.train.get_checkpoint_state(checkpoint_path)\n",
        "\n",
        "            if checkpoint and checkpoint.model_checkpoint_path:\n",
        "                saver = tf.train.import_meta_graph(\n",
        "                    checkpoint.model_checkpoint_path + \".meta\"\n",
        "                )\n",
        "                saver.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
        "\n",
        "            for index in range(epochs):\n",
        "                # train by batch\n",
        "                batch_features, batch_labels = train_data.next_batch(self.batch_size)\n",
        "                batch_labels[batch_labels == 0] = -1\n",
        "\n",
        "                # input dictionary with dropout of 50%\n",
        "                feed_dict = {\n",
        "                    self.x_input: batch_features,\n",
        "                    self.y_input: batch_labels,\n",
        "                    self.keep_prob: 0.5,\n",
        "                }\n",
        "\n",
        "                # run the train op\n",
        "                summary, _, loss = sess.run(\n",
        "                    [self.merged, self.optimizer, self.loss], feed_dict=feed_dict\n",
        "                )\n",
        "\n",
        "                # every 100th step and at 0,\n",
        "                if index % 100 == 0:\n",
        "                    feed_dict = {\n",
        "                        self.x_input: batch_features,\n",
        "                        self.y_input: batch_labels,\n",
        "                        self.keep_prob: 1.0,\n",
        "                    }\n",
        "\n",
        "                    # get the accuracy of training\n",
        "                    train_accuracy = sess.run(self.accuracy, feed_dict=feed_dict)\n",
        "\n",
        "                    # display the training accuracy\n",
        "                    print(\n",
        "                        \"step: {}, training accuracy : {}, training loss : {}\".format(\n",
        "                            index, train_accuracy, loss\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    train_writer.add_summary(summary=summary, global_step=index)\n",
        "\n",
        "                    saver.save(\n",
        "                        sess,\n",
        "                        save_path=os.path.join(checkpoint_path, self.name),\n",
        "                        global_step=index,\n",
        "                    )\n",
        "\n",
        "            test_features = test_data.images\n",
        "            test_labels = test_data.labels\n",
        "            test_labels[test_labels == 0] = -1\n",
        "\n",
        "            feed_dict = {\n",
        "                self.x_input: test_features,\n",
        "                self.y_input: test_labels,\n",
        "                self.keep_prob: 1.0,\n",
        "            }\n",
        "\n",
        "            test_accuracy = sess.run(self.accuracy, feed_dict=feed_dict)\n",
        "\n",
        "            print(\"Test Accuracy: {}\".format(test_accuracy))\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_variable(shape):\n",
        "        \"\"\"Returns a weight matrix consisting of arbitrary values.\n",
        "        :param shape: The shape of the weight matrix to create.\n",
        "        :return: The weight matrix consisting of arbitrary values.\n",
        "        \"\"\"\n",
        "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "        return tf.Variable(initial)\n",
        "\n",
        "    @staticmethod\n",
        "    def bias_variable(shape):\n",
        "        \"\"\"Returns a bias matrix consisting of 0.1 values.\n",
        "        :param shape: The shape of the bias matrix to create.\n",
        "        :return: The bias matrix consisting of 0.1 values.\n",
        "        \"\"\"\n",
        "        initial = tf.constant(0.1, shape=shape)\n",
        "        return tf.Variable(initial)\n",
        "\n",
        "    @staticmethod\n",
        "    def conv2d(features, weight):\n",
        "        \"\"\"Produces a convolutional layer that filters an image subregion\n",
        "        :param features: The layer input.\n",
        "        :param weight: The size of the layer filter.\n",
        "        :return: Returns a convolutional layer.\n",
        "        \"\"\"\n",
        "        return tf.nn.conv2d(features, weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "\n",
        "    @staticmethod\n",
        "    def max_pool_2x2(features):\n",
        "        \"\"\"Downnsamples the image based on convolutional layer\n",
        "        :param features: The input to downsample.\n",
        "        :return: Downsampled input.\n",
        "        \"\"\"\n",
        "        return tf.nn.max_pool(\n",
        "            features, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\"\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "cGg9T1CwKTJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model.snn_softmax\n",
        "\n",
        "#Class: CNN"
      ],
      "metadata": {
        "id": "bWZb_Rn4KYG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN:\n",
        "    def __init__(self, alpha, batch_size, num_classes, num_features):\n",
        "        \"\"\"Initializes the CNN-Softmax model\n",
        "        :param alpha: The learning rate to be used by the model.\n",
        "        :param batch_size: The number of batches to use for training/validation/testing.\n",
        "        :param num_classes: The number of classes in the dataset.\n",
        "        :param num_features: The number of features in the dataset.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.batch_size = batch_size\n",
        "        self.name = \"CNN-Softmax\"\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = num_features\n",
        "\n",
        "        def __graph__():\n",
        "\n",
        "            with tf.name_scope(\"input\"):\n",
        "                # [BATCH_SIZE, NUM_FEATURES]\n",
        "              x_input = tf.placeholder(\n",
        "                  dtype=float, shape=[None, num_features], name=\"x_input\"\n",
        "              )\n",
        "\n",
        "                # [BATCH_SIZE, NUM_CLASSES]\n",
        "              y_input = tf.placeholder(\n",
        "                  dtype=float, shape=[None, num_classes], name=\"actual_label\"\n",
        "              )\n",
        "\n",
        "            # First convolutional layer\n",
        "            first_conv_weight = self.weight_variable([5, 5, 1, 32])\n",
        "            first_conv_bias = self.bias_variable([32])\n",
        "\n",
        "            input_image = tf.reshape(x_input, [-1, 28, 28, 1])\n",
        "\n",
        "            first_conv_activation = tf.nn.relu(\n",
        "                self.conv2d(input_image, first_conv_weight) + first_conv_bias\n",
        "            )\n",
        "            first_conv_pool = self.max_pool_2x2(first_conv_activation)\n",
        "\n",
        "            # Second convolutional layer\n",
        "            second_conv_weight = self.weight_variable([5, 5, 32, 64])\n",
        "            second_conv_bias = self.bias_variable([64])\n",
        "\n",
        "            second_conv_activation = tf.nn.relu(\n",
        "                self.conv2d(first_conv_pool, second_conv_weight) + second_conv_bias\n",
        "            )\n",
        "            second_conv_pool = self.max_pool_2x2(second_conv_activation)\n",
        "\n",
        "            # Fully-connected layer (Dense Layer)\n",
        "            dense_layer_weight = self.weight_variable([7 * 7 * 64, 1024])\n",
        "            dense_layer_bias = self.bias_variable([1024])\n",
        "\n",
        "            second_conv_pool_flatten = tf.reshape(second_conv_pool, [-1, 7 * 7 * 64])\n",
        "            dense_layer_activation = tf.nn.relu(\n",
        "                tf.matmul(second_conv_pool_flatten, dense_layer_weight)\n",
        "                + dense_layer_bias\n",
        "            )\n",
        "\n",
        "            # Dropout, to avoid over-fitting\n",
        "            keep_prob = tf.placeholder(tf.float32)\n",
        "            h_fc1_drop = tf.nn.dropout(dense_layer_activation, keep_prob)\n",
        "\n",
        "            # Readout layer\n",
        "            readout_weight = self.weight_variable([1024, num_classes])\n",
        "            readout_bias = self.bias_variable([num_classes])\n",
        "\n",
        "            output = tf.matmul(h_fc1_drop, readout_weight) + readout_bias\n",
        "\n",
        "            with tf.name_scope(\"softmax\"):\n",
        "                loss = tf.reduce_mean(\n",
        "                    tf.nn.softmax_cross_entropy_with_logits(\n",
        "                        logits=output, labels=y_input\n",
        "                    )\n",
        "                )\n",
        "            tf.summary.scalar(\"loss\", loss)\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=alpha).minimize(loss)\n",
        "\n",
        "            with tf.name_scope(\"accuracy\"):\n",
        "                output = tf.identity(tf.nn.softmax(output), name=\"prediction\")\n",
        "                correct_prediction = tf.equal(\n",
        "                    tf.argmax(output, 1), tf.argmax(y_input, 1)\n",
        "                )\n",
        "                with tf.name_scope(\"accuracy\"):\n",
        "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "            tf.summary.scalar(\"accuracy\", accuracy)\n",
        "\n",
        "            merged = tf.summary.merge_all()\n",
        "\n",
        "            self.x_input = x_input\n",
        "            self.y_input = y_input\n",
        "            self.keep_prob = keep_prob\n",
        "            self.output = output\n",
        "            self.loss = loss\n",
        "            self.optimizer = optimizer\n",
        "            self.accuracy = accuracy\n",
        "            self.merged = merged\n",
        "\n",
        "        sys.stdout.write(\"\\n<log> Building graph...\")\n",
        "        __graph__()\n",
        "        sys.stdout.write(\"</log>\\n\")\n",
        "\n",
        "    def train(self, checkpoint_path, epochs, log_path, train_data, test_data):\n",
        "        \"\"\"Trains the initialized model.\n",
        "        :param checkpoint_path: The path where to save the trained model.\n",
        "        :param epochs: The number of passes through the entire dataset.\n",
        "        :param log_path: The path where to save the TensorBoard logs.\n",
        "        :param train_data: The training dataset.\n",
        "        :param test_data: The testing dataset.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        if not os.path.exists(path=log_path):\n",
        "            os.mkdir(log_path)\n",
        "\n",
        "        if not os.path.exists(path=checkpoint_path):\n",
        "            os.mkdir(checkpoint_path)\n",
        "\n",
        "        saver = tf.train.Saver(max_to_keep=4)\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        timestamp = str(time.asctime())\n",
        "\n",
        "        train_writer = tf.summary.FileWriter(\n",
        "            logdir=log_path + timestamp + \"-training\", graph=tf.get_default_graph()\n",
        "        )\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            checkpoint = tf.train.get_checkpoint_state(checkpoint_path)\n",
        "\n",
        "            if checkpoint and checkpoint.model_checkpoint_path:\n",
        "                saver = tf.train.import_meta_graph(\n",
        "                    checkpoint.model_checkpoint_path + \".meta\"\n",
        "                )\n",
        "                saver.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
        "\n",
        "            for index in range(epochs):\n",
        "                # train by batch\n",
        "                batch_features, batch_labels = train_data.next_batch(self.batch_size)\n",
        "\n",
        "                # input dictionary with dropout of 50%\n",
        "                feed_dict = {\n",
        "                    self.x_input: batch_features,\n",
        "                    self.y_input: batch_labels,\n",
        "                    self.keep_prob: 0.5,\n",
        "                }\n",
        "\n",
        "                # run the train op\n",
        "                summary, _, loss = sess.run(\n",
        "                    [self.merged, self.optimizer, self.loss], feed_dict=feed_dict\n",
        "                )\n",
        "\n",
        "                # every 100th step and at 0,\n",
        "                if index % 100 == 0:\n",
        "                    feed_dict = {\n",
        "                        self.x_input: batch_features,\n",
        "                        self.y_input: batch_labels,\n",
        "                        self.keep_prob: 1.0,\n",
        "                    }\n",
        "\n",
        "                    # get the accuracy of training\n",
        "                    train_accuracy = sess.run(self.accuracy, feed_dict=feed_dict)\n",
        "\n",
        "                    # display the training accuracy\n",
        "                    print(\n",
        "                        \"step: {}, training accuracy : {}, training loss : {}\".format(\n",
        "                            index, train_accuracy, loss\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    train_writer.add_summary(summary=summary, global_step=index)\n",
        "\n",
        "                    saver.save(\n",
        "                        sess,\n",
        "                        save_path=os.path.join(checkpoint_path, self.name),\n",
        "                        global_step=index,\n",
        "                    )\n",
        "\n",
        "            test_features = test_data.images\n",
        "            test_labels = test_data.labels\n",
        "\n",
        "            feed_dict = {\n",
        "                self.x_input: test_features,\n",
        "                self.y_input: test_labels,\n",
        "                self.keep_prob: 1.0,\n",
        "            }\n",
        "\n",
        "            test_accuracy = sess.run(self.accuracy, feed_dict=feed_dict)\n",
        "\n",
        "            print(\"Test Accuracy: {}\".format(test_accuracy))\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_variable(shape):\n",
        "        \"\"\"Returns a weight matrix consisting of arbitrary values.\n",
        "        :param shape: The shape of the weight matrix to create.\n",
        "        :return: The weight matrix consisting of arbitrary values.\n",
        "        \"\"\"\n",
        "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "        return tf.Variable(initial)\n",
        "\n",
        "    @staticmethod\n",
        "    def bias_variable(shape):\n",
        "        \"\"\"Returns a bias matrix consisting of 0.1 values.\n",
        "        :param shape: The shape of the bias matrix to create.\n",
        "        :return: The bias matrix consisting of 0.1 values.\n",
        "        \"\"\"\n",
        "        initial = tf.constant(0.1, shape=shape)\n",
        "        return tf.Variable(initial)\n",
        "\n",
        "    @staticmethod\n",
        "    def conv2d(features, weight):\n",
        "        \"\"\"Produces a convolutional layer that filters an image subregion\n",
        "        :param features: The layer input.\n",
        "        :param weight: The size of the layer filter.\n",
        "        :return: Returns a convolutional layer.\n",
        "        \"\"\"\n",
        "        return tf.nn.conv2d(features, weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "\n",
        "    @staticmethod\n",
        "    def max_pool_2x2(features):\n",
        "        \"\"\"Downnsamples the image based on convolutional layer\n",
        "        :param features: The input to downsample.\n",
        "        :return: Downsampled input.\n",
        "        \"\"\"\n",
        "        return tf.nn.max_pool(\n",
        "            features, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "0ZvaY53WKgLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#main.py"
      ],
      "metadata": {
        "id": "I1rq7NV9K7Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.14\n"
      ],
      "metadata": {
        "id": "ssYW8-OGNZF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9165d160-eb3e-4803-87e8-d066f4bc6898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==1.14 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.37.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.21.5)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.44.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow_datasets\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import argparse\n",
        "import numpy as nm"
      ],
      "metadata": {
        "id": "t8Q_QwIWMgtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mnist = tensorflow_datasets.load('mnist')\n",
        "mnist = input_data.read_data_sets('./MNIST_data', one_hot=True)\n",
        "num_classes = mnist.train.labels.shape[1]\n",
        "sequence_length = mnist.train.images.shape[1]\n",
        "#model_choice = 1"
      ],
      "metadata": {
        "id": "mRZdbIfeLAGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25aa1c6b-ff10-4bdd-8cf5-d608c730e2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-0725fba9f268>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bkjE8FqUop0",
        "outputId": "61155eac-4ba9-40fb-fc1d-1c0e0eb59d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = CNNSVM(\n",
        "#             alpha=1e-3,\n",
        "#             batch_size=128,\n",
        "#             num_classes=num_classes,\n",
        "#             num_features=sequence_length,\n",
        "#             penalty_parameter = 1\n",
        "#         )\n",
        "# model.train(\n",
        "#             checkpoint_path='./checkpointSVM1',\n",
        "#             epochs=1000,\n",
        "#             log_path='./logsSVM1',\n",
        "#             train_data=mnist.train,\n",
        "#             test_data=mnist.test,\n",
        "#         )"
      ],
      "metadata": {
        "id": "h7-uead9PFAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = CNN(\n",
        "            alpha=1e-3,\n",
        "            batch_size=128,\n",
        "            num_classes=num_classes,\n",
        "            num_features=sequence_length,\n",
        "        )\n",
        "model2.train(\n",
        "            checkpoint_path='./checkpoint3',\n",
        "            epochs=1000,\n",
        "            log_path='./log3',\n",
        "            train_data=mnist.train,\n",
        "            test_data=mnist.test,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTeavqG7RrfD",
        "outputId": "e82fd5ed-be65-4e23-9632-3a7737018968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<log> Building graph...WARNING:tensorflow:From <ipython-input-4-b7cce45bf2fe>:60: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-4-b7cce45bf2fe>:71: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "</log>\n",
            "step: 0, training accuracy : 0.328125, training loss : 10.906167984008789\n",
            "step: 100, training accuracy : 0.9765625, training loss : 0.12954172492027283\n",
            "step: 200, training accuracy : 0.96875, training loss : 0.16694125533103943\n",
            "step: 300, training accuracy : 1.0, training loss : 0.03546420484781265\n",
            "step: 400, training accuracy : 0.9765625, training loss : 0.07342566549777985\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "step: 500, training accuracy : 1.0, training loss : 0.07240831851959229\n",
            "step: 600, training accuracy : 0.984375, training loss : 0.10311383008956909\n",
            "step: 700, training accuracy : 0.984375, training loss : 0.0761517584323883\n",
            "step: 800, training accuracy : 0.984375, training loss : 0.04284524917602539\n",
            "step: 900, training accuracy : 0.9921875, training loss : 0.07565468549728394\n",
            "Test Accuracy: 0.9873999953269958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rmnsXfSzQP1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IDz2MMBFdGQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}